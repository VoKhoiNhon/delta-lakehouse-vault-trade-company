model:
  table_name: "silver.s_person_address"
  database_name: "lakehouse-silver"
  data_location: "s3a://lakehouse-silver{{env}}/s_person_address"
  unique_key: ["jurisdiction", "dv_hashkey_person"]
  data_format: "delta"
  partition_by:
    - jurisdiction

  columns:
    - name: dv_hashkey_person
      type: string
      nullable: false
      description: dv hashkey
    - name: dv_recsrc
      type: string
      nullable: false
      description: dv columns
    - name: dv_loaddts
      nullable: false
      type: timestamp
      description: dv columns

    - name: dv_hashdiff
      nullable: false
      type: string
      description: dv columns
    - name: dv_source_version
      type: string
      description: dv columns

    - name: jurisdiction
      type: string
    - name: name
      type: string
    - name: full_address
      type: string

    - name: street
      type: string
      nullable: true
    - name: city
      type: string
      nullable: true
    - name: region
      type: string
      nullable: true
    - name: state
      type: string
      nullable: true
    - name: province
      type: string
      nullable: true
    - name: country_code
      type: string
      nullable: true
    - name: country_name
      type: string
      nullable: true
    - name: postal_code
      type: string
      nullable: true
    - name: latitude
      type: float
      nullable: true
    - name: longitude
      type: float
      nullable: true
    - name: type
      type: integer
      nullable: false
    - name: start_date
      type: date
    - name: end_date
      type: date

input_resources:
  # 1tm
  - table_name: 1tm_2412.person
    format: delta
    data_location: s3a://lakehouse-bronze/1tm_2412/person
    record_source: 1tm
    process_job: jobs/silver/s_person_demographic/s_person_address__src_1tm_person.py

  ##
  - table_name: bronze.kentucky
    format: delta
    data_location: s3a://lakehouse-bronze/kentucky/company
    record_source: https://web.sos.ky.gov/BusSearchNProfile/search.aspx
    process_job: jobs/silver/s_person_address/s_person_address__src_kentucky.py

  - table_name: bronze.opensanction
    format: delta
    data_location: s3a://lakehouse-bronze/sanction
    record_source: https://www.opensanctions.org/datasets/default/
    process_job: jobs/silver/s_person_address/s_person_address__src_opensanction.py


  ##################
  # Bronze Sanction
  ##################
  - table_name: "bronze.sanction.person"
    format: "delta"
    data_location: "s3a://lakehouse-bronze{{env}}/sanction/person"
    record_source: "https://www.opensanctions.org/datasets/default/"
    process_job: "jobs/silver/s_person_demographic/s_person_demographic__src_sanction.py"
    dv_source_version: "gov_sanction_person;{{load_date}}"

#################
# Spark Resources
#################
spark_resources:
  # Custom Resource
  spark.driver.memory  : "{{spark_driver_memory|default('10g')}}"
  spark.executor.memory: "{{spark_executor_memory|default('32g')}}"
  spark.executor.cores : "{{spark_executor_cores|default(8)}}"
  spark.executor.instances : "{{spark_executor_instances|default(1)}}"
  spark.driver.memoryOverhead: "{{spark_driver_memoryOverhead|default('512mb')}}"
  spark.executor.memoryOverhead: "{{spark_executor_memoryOverhead|default('10g')}}"
  spark.dynamicAllocation.enabled: "true"
  spark.dynamicAllocation.minExecutors: "1"
  spark.dynamicAllocation.maxExecutors: "{{spark_dynamicAllocation_maxExecutors|default(1)}}"
  spark.sql.autoBroadcastJoinThreshold: "{{spark_sql_autoBroadcastJoinThreshold|default('100MB')}}"

  spark.sql.execution.arrow.pyspark.enabled: "true"
  spark.sql.files.openCostInBytes: "134217728"
  spark.sql.files.maxPartitionBytes: "256MB"

  # Fault Tolerance
  spark.executor.heartbeatInterval: "60s"
  spark.network.timeout: "900s"
  spark.task.maxFailures: "10"
  spark.stage.maxConsecutiveAttempts: "4"

  spark.databricks.delta.snapshotCache.validation.enabled: "false"
  spark.databricks.delta.optimize.snapshotRead.enabled : "true"
  spark.databricks.delta.retentionDurationCheck.enabled: "false"
  # spark.databricks.delta.autoCompact.enabled: "true"
  spark.databricks.delta.optimizeWrite.enabled: "true"
  spark.delta.targetFileSize: "{{spark_delta_targetFileSize|default('256mb')}}"
