model:
  table_name: "silver.s_sanction"
  database_name: "lakehouse-silver"
  data_location: "s3a://lakehouse-silver{{env}}/s_sanction"
  unique_key: ["dv_hashkey_sanction"]
  data_format: "delta"
  partition_by:
    - "jurisdiction"

  columns:
  - name: dv_hashkey_sanction
    type: string
    nullable: false
    description: dv hashkey
  - name: dv_hashdiff
    nullable: false
    type: string
    description: dv columns
  - name: dv_recsrc
    type: string
    nullable: false
    description: dv columns
  - name: dv_source_version
    type: string
    description: dv columns
  - name: dv_loaddts
    nullable: false
    type: timestamp
    description: dv columns
  - name: id
    type: string
  - name: sanction_id
    type: string
  - name: source_schema
    type: string
  - name: authority
    type: string
  - name: country
    type: string
  - name: program
    type: string
  - name: reason
    type: string
  - name: start_date
    type: string
  - name: end_date
    type: string
  - name: source_link
    type: string
  - name: jurisdiction
    type: string

input_resources:
  - table_name: bronze.sanction
    format: delta
    data_location: s3a://lakehouse-bronze/1tm_2412/sanction
    record_source: bronze.sanction
    process_job: jobs/silver/s_sanction/s_sanction__src_1tm_2412.py

  ##################
  # Bronze Sanction
  ##################
  - table_name: "bronze.sanction.person"
    format: "delta"
    data_location: "s3a://lakehouse-bronze{{env}}/sanction/person"
    record_source: "https://www.opensanctions.org/datasets/default/"
    process_job: "jobs/silver/h_sanction/h_sanction__src_sanction.py"
    dv_source_version: "gov_sanction_person;{{load_date}}"
    filter: "is_sanctioned = 1"
  - table_name: "bronze.sanction.company"
    format: "delta"
    data_location: "s3a://lakehouse-bronze{{env}}/sanction/company"
    record_source: "https://www.opensanctions.org/datasets/default/"
    process_job: "jobs/silver/h_sanction/h_sanction__src_sanction.py"
    dv_source_version: "gov_sanction_company;{{load_date}}"
    filter: "is_sanctioned = 1"
  - table_name: "bronze.sanction.sanction"
    format: "delta"
    data_location: "s3a://lakehouse-bronze{{env}}/sanction/sanction"
    record_source: "https://www.opensanctions.org/datasets/default/"
    process_job: "jobs/silver/h_sanction/h_sanction__src_sanction.py"
    dv_source_version: "gov_sanction_sanction;{{load_date}}"

#################
# Spark Resources
#################
spark_resources:
  # Custom Resource
  spark.driver.memory  : "{{spark_driver_memory|default('10g')}}"
  spark.executor.memory: "{{spark_executor_memory|default('32g')}}"
  spark.executor.cores : "{{spark_executor_cores|default(8)}}"
  spark.executor.instances : "{{spark_executor_instances|default(1)}}"
  spark.driver.memoryOverhead: "{{spark_driver_memoryOverhead|default('512mb')}}"
  spark.executor.memoryOverhead: "{{spark_executor_memoryOverhead|default('10g')}}"
  spark.dynamicAllocation.enabled: "true"
  spark.dynamicAllocation.minExecutors: "1"
  spark.dynamicAllocation.maxExecutors: "{{spark_dynamicAllocation_maxExecutors|default(1)}}"
  spark.sql.autoBroadcastJoinThreshold: "{{spark_sql_autoBroadcastJoinThreshold|default('100MB')}}"

  spark.sql.execution.arrow.pyspark.enabled: "true"
  spark.sql.files.openCostInBytes: "134217728"
  spark.sql.files.maxPartitionBytes: "256MB"

  # Fault Tolerance
  spark.executor.heartbeatInterval: "60s"
  spark.network.timeout: "900s"
  spark.task.maxFailures: "10"
  spark.stage.maxConsecutiveAttempts: "4"

  spark.databricks.delta.snapshotCache.validation.enabled: "false"
  spark.databricks.delta.optimize.snapshotRead.enabled : "true"
  spark.databricks.delta.retentionDurationCheck.enabled: "false"
  # spark.databricks.delta.autoCompact.enabled: "true"
  spark.databricks.delta.optimizeWrite.enabled: "true"
  spark.delta.targetFileSize: "{{spark_delta_targetFileSize|default('256mb')}}"
